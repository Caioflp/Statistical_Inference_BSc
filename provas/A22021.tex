\documentclass[a4paper,10pt, notitlepage]{report}
\usepackage[utf8]{inputenc}
\usepackage{natbib}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{cancel}
\usepackage{mathtools}
\usepackage[portuguese]{babel}

%%%%%%%%%%%%%%%%%%%% Notation stuff
\newcommand{\pr}{\operatorname{Pr}} %% probability
\newcommand{\vr}{\operatorname{Var}} %% variance
\newcommand{\rs}{X_1, X_2, \ldots, X_n} %%  random sample
\newcommand{\irs}{X_1, X_2, \ldots} %% infinite random sample
\newcommand{\rsd}{x_1, x_2, \ldots, x_n} %%  random sample, realised
\newcommand{\bX}{\boldsymbol{X}} %%  random sample, contracted form (bold)
\newcommand{\bx}{\boldsymbol{x}} %%  random sample, realised, contracted form (bold)
\newcommand{\bT}{\boldsymbol{T}} %%  Statistic, vector form (bold)
\newcommand{\bt}{\boldsymbol{t}} %%  Statistic, realised, vector form (bold)
\newcommand{\emv}{\hat{\theta}}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

% Title Page
\title{Segunda avaliação (A2)}
\author{Disciplina: Inferência Estatística \\ Professor: Luiz Max Carvalho}
\date{29 de Novembro de 2021}

\begin{document}
\maketitle

% \textbf{Data de Entrega: 19 de Agosto de 2020.}

\begin{center}
\fbox{\fbox{\parbox{1.0\textwidth}{\textsf{
    \begin{itemize}
        \item Por favor, entregue um único arquivo PDF;
        \item O tempo para realização da prova é de 3 horas, mais vinte minutos para upload do documento para o e-class;
        \item Leia a prova toda com calma antes de começar a responder;
        \item Responda todas as questões sucintamente;
        \item Marque a resposta final claramente com um quadrado, círculo ou figura geométrica de sua preferência;
        \item A prova vale 80 pontos. A pontuação restante é contada como bônus;
        \item Lembre-se de consultar o catálogo de fórmulas no fim deste documento.
    \end{itemize}}
}}}
\end{center}
\newpage
\section*{Dicas}
\begin{itemize}
 \item Se $Y \sim \operatorname{Poisson}(\lambda)$ então, para $\lambda$ grande o suficiente, temos
 \begin{itemize}
  \item $E[\sqrt{Y} + \sqrt{Y+1}] \approx \sqrt{4\lambda + 1}$;
  \item $\vr(\sqrt{Y} + \sqrt{Y+1}) \approx 1$. 
 \end{itemize} 
 Ademais, temos
 \begin{equation*}
 \frac{Y-\lambda}{\sqrt{\lambda}} \to \operatorname{Normal}(0, 1),
 \end{equation*}
 onde a seta representa convergência em distribuição.
 \item Se $X$ tem distribuição Poisson com média $\lambda$, então
 \[\pr\left(X \leq x\right) = Q(\floor*{x + 1}, \lambda), \]
 onde $$Q(x, s) = \frac{\Gamma(x, s)}{\Gamma(x)}$$ é a função Gama regularizada superior e $\floor*{y}$ é maior inteiro menor ou igual a $y$ -- também chamado de~\textit{floor}.
 Ademais, temos
\[ \frac{\partial}{\partial s} Q(x, s) = -\frac{e^{-s}s^{x-1}}{\Gamma(x)},\]
onde $\Gamma(x) = (x-1)!$ é a função Gamma.
\item  Se $\rs$ é uma amostra aleatória, então
\begin{equation*}
 \pr\left( \max(\rs) \leq m \right) = \prod_{i=1}^n \pr(X_i \leq m).
\end{equation*}
 \item Em uma regressão linear simples, temos:
  \begin{align*}
  \hat{\beta_0} &\sim \operatorname{Normal}\left(\beta_0, \sigma^2 \left( \frac{1}{n} + \frac{\bar{x}^2}{s_x^2} \right) \right),\\
  \hat{\beta_1}  &\sim \operatorname{Normal}\left(\beta_1, \frac{\sigma^2}{s_x^2}\right),\\
  &\operatorname{Cov}\left(\hat{\beta_0}, \hat{\beta_1} \right)  = -\frac{\bar{x}\sigma^2}{s_x^2},
 \end{align*}
 onde $s_x = \sqrt{\sum_{i=1}^n (x_i-\bar{x})^2}$ e $\hat{\beta_0}$ e $\hat{\beta_1}$ são os estimadores de máxima verossimilhança dos coeficientes.
 \end{itemize}
 
\newpage

\section*{1. A duck row}

Tio Patinhas e seus sobrinhos continuam sua jornada no aprendizado da Estatística e  consideram  $\rs \sim \operatorname{Uniforme}(0, \theta)$, $\theta > 0$, uma amostra aleatória com $M := \max(\rs)$.
Várias dúvidas e discordâncias surgiram entre os patinhos, no entanto. 
No que se segue, você deve utilizar seus conhecimentos de Estatística para esclarecer as coisas.


\begin{enumerate}[label=\alph*)]
\item (5 pontos) Huguinho diz que $Y = \theta^2 M$ é pivotal, Zezinho discorda.
Mostre que Huguinho está equivocado e encontre uma quantidade pivotal.
 \item (5 pontos) Mostre ao Pato Donald como utilizar a quantidade obtida no item anterior para obter um intervalo de confiança unilateral exato de $100\gamma \%$ para $\theta$. 
 \item (5 pontos) Defina\footnote{Preste atenção à formulação das hipóteses!} as hipóteses $H_0: \theta = 1/3$ e $H_1: \theta > 1/3$. 
Suponha que propomos um  procedimento de teste $\delta_c$ que rejeita $H_0$ quando $M > c$.
Deduza a função poder do teste e encontre $c$ de modo que o teste tenha tamanho $\alpha_0$.
 \item (10 pontos) Luisinho cismou que o  este teste tem razão de verossimilhanças monótona. 
 Ele está certo?
 Justifique sua resposta.
  \item (10 pontos) Tio Patinhas jura que este teste é uniformemente mais poderoso.
  Explique para Zezinho o que isso significa e depois discuta se o Tio Patinhas tem razão.
  Justifique sua resposta.
\end{enumerate}

\section*{2. The God(damn) particle}

O bóson de Higgs\footnote{Assim nomeado em homenagem ao físico inglês Peter Higgs (1929-).}, que confere massa a partículas que de outra forma não teriam massa, foi a última partícula do chamado Modelo Padrão (\textit{standard model}) da Física Quântica a ter sua existência confirmada experimentalmente.

A ideia fundamental da descoberta experimental de novas partículas é a de comparar o que se espera sob um modelo base sem a nova partícula e um modelo proposto que a inclui.
Uma maneira de fazer isso é detectar o número $X$ de ocorrências (eventos) de decaimento de bósons e comparar esse número com o que é esperado sob o modelo mais simples.
Como o medidor é imperfeito, considere o seguinte modelo:
\begin{equation*}
 X \sim \operatorname{Poisson}(\beta  + \kappa\mu),
\end{equation*}
com $\mu >0$, onde vamos assumir que a taxa base $\beta > 0$ e a taxa esperada do bóson de Higgs $\kappa >0$ são conhecidas.
Desta forma, o modelo com $\mu = 0$ é o modelo de base e aquele onde $\mu =1$ é o modelo com o bóson de Higgs.

\begin{enumerate}[label=\alph*)]
 \item (10 pontos) Encontre o EMV de $\mu$ e comente sobre a sua adequação; o EMV sempre produz estimativas válidas? 
  \item (10 pontos) Encontre um intervalo de confiança bilateral de $100\gamma\%$ \textbf{aproximado} para $\mu$;
  
  \textit{Dica}: Você pode assumir que $\beta$ é grande o suficiente. Use as dicas no começo da prova.
  \item (10 pontos) Considere um teste em que assumimos que $X\sim\operatorname{Poisson}(\theta)$ e construímos um teste $\delta_c$ de modo que, se $X \geq c$, rejeitamos $H_0: \theta \leq \theta_0$.
  \begin{itemize}
  \item Mostre que o teste em questão é não-viesado.
   \item Escreva $\theta_0$ em função de $\beta$, $\kappa$ e $\mu$ de modo a testar um desvio em relação ao modelo base.
   
   \textit{Dica}: Existem várias respostas certas.   
  \end{itemize}
 \item (10 pontos) Formule um teste para $H_0: \mu =0$ vs $H_1: \mu > 0$ como um teste de razão de verossimilhanças;
\end{enumerate}


\section*{3. Gosto muito de você, linearzinho.}

O modelo linear (de regressão) é um dos cavalos de batalha da Estatística, sendo aplicado em problemas de Finanças, Medicina e Engenharia.
Vamos agora estudar como utilizar as propriedades deste modelo para desenhar experimentos com garantias matemáticas de desempenho e obter estimadores de quantidades de interesse.

\begin{enumerate}[label=\alph*)]
 \item (5 pontos) Uma prática comum em regressão é a de \textbf{centrar} a variável independente (covariável), isto é subtrair a média; isto facilita a interpretação do intercepto e também simplifica alguns cálculos importantes.
 Mostre que no caso com a covariável centrada, $\hat{\beta_0}$ e $\hat{\beta_1}$ são independentes;
 \item (7,5 pontos) Mais uma vez considerando o caso centrado, mostre 
 como obter o número de observações $n$ que faz com que a variância do estimador de máxima verossimilhança do intercepto seja menor que $v > 0$;
 \item (5 pontos) Mostre como obter um estimador não-viesado da quantidade $\theta = a\beta_0 + b\beta_1 + c$, com $a, b, c \neq 0$, e encontre o seu erro quadrático médio.
 \item (7,5 pontos)  Quando $x_{\text{pred}} = \bar{x}$, mostre como obter  o número de observações $n$ necessário para que o intervalo de predição de $100(1-\alpha_0)\%$ para a variável-resposta ($Y$) tenha largura menor ou igual a $l>0$ com probabilidade pelo menos $\gamma$.
 
 \textit{Dicas}:(i) A expressão dependerá~\textit{também} da variância dos resíduos, $\sigma^2$ e (ii) Você não precisa calcular $n$, apenas mostrar o procedimento para obtê-lo.
 
 \end{enumerate}

\newpage
\section*{Fórmulas úteis}
\textbf{Como usar este catálogo:} as fórmulas dadas aqui estão propositalmente privadas do seu contexto.
O objetivo desta coleção é ajudar você a lembrar das expressões.
Entretanto, saber quais expressões são utilizadas em que contexto é sua tarefa.
\begin{itemize}
 \item $ \bar{X}_n = \frac{1}{n} \sum_{i=1}^n X_i$;
 \item $\hat{\sigma}^\prime = \sqrt{\frac{1}{n-1}\sum_{i=1}^n \left(X_i - \bar{X}_n\right)^2}$;
 \item $S_X^2 = \sum_{i=1}^m (X_i-\bar{X}_m)^2$;
 \item $S_Y^2 = \sum_{j=1}^n (Y_j-\bar{Y}_n)^2$;
 \item $U = \frac{\sqrt{m + n - 2}(\bar{X}_m - \bar{Y}_n)}{\sqrt{\left(\frac{1}{m} + \frac{1}{n}\right) (S_X^2 + S_Y^2)}}$;
 \item $ V = \frac{S_X^2/(m-1)}{S_Y^2/(n-1)}$;
 \item $\bar{x} = (1/n)\sum_{i=1}^n X_i$;
 \item $\bar{y} = (1/n)\sum_{i=1}^n Y_i$;
\item \begin{equation*}
        E\left[\left(\hat{Y} - Y\right)^2\right] = \sigma^2 \left(1 + \frac{1}{n} + \frac{\left(x_{\text{pred}}-\bar{x}\right)^2}{s_x^2}\right).
      \end{equation*}
  \item \begin{equation*}
 \hat{\sigma}_r^\prime := \sqrt{\frac{\sum_{i=1}^n \left(Y_i - \hat{\beta_0} - \hat{\beta_1}x_i \right)^2}{n-2}}.
\end{equation*}     
\item \begin{align*}
 &\hat{\beta_0} \pm \hat{\sigma}^\prime c\sqrt{\frac{1}{n} + \frac{\bar{x}^2}{s_x^2}}\quad \text{e}\quad \hat{\beta_1} \pm c\frac{\hat{\sigma}^\prime}{s_x},\\
 &\hat{\beta_0} + \hat{\beta_1}x_{\text{pred}} \pm c \hat{\sigma}^\prime \sqrt{\frac{1}{n} + \frac{\left(x_{\text{pred}}-\bar{x}\right)^2}{s_x^2} },
\end{align*}    
\item \begin{equation*}
 \hat{Y} \pm c\hat{\sigma}_r^\prime \sqrt{\left[ 1+ \frac{1}{n} + \frac{\left(x_{\text{pred}}-\bar{x}\right)^2}{s_x^2} \right]},
\end{equation*}
onde $c = T^{-1}(1-\frac{\alpha_0}{2}; n-2)$.
\end{itemize}

\end{document}          
